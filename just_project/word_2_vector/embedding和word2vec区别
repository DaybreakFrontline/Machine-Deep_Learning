曾经提到的对汉字做"embedding"，和这里所说的word2vec有什么区别？

事实上，不管是在训练RNN时，还是在训练word2vec模型，都是加入了一个"词嵌入层"，只不过对象有所不同，
一个是汉字，一个是英文单词。这个词嵌入层可以把输入的汉字或英文单词嵌入到一个更稠密的空间中，
这有助于模型性能的提升。

训练的方式有所不同，在训练RNN时，采样RNN的损失，通过预测下一个时刻的字符来训练模型，顺带着得到了词嵌入。
这里采用Skip-Gram方法，通过预测单词的上下文来训练词嵌入。

如果要训练一个以单词为输入单位的RNN（即模型的每一步的输入都是单词，输入的每一步也是单词，而不是字母）
那么可以用这里训练得到的词嵌入作为要训练的RNN的词嵌入层的初始值，这样做可以大大提高收敛速度。
